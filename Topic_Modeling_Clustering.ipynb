{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ahSYG7xQNws"
      },
      "source": [
        "# **EXTRACTING TEXT AND CREATING JSON FROM NEW FILE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "iRweR3Xkx3wO",
        "outputId": "629544c0-63b4-4f0c-c1d4-3b1229d5409d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDF\n",
            "Successfully installed PyMuPDF-1.25.5\n",
            "Collecting Pix2Text\n",
            "  Downloading pix2text-1.1.2.3-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from Pix2Text) (8.1.8)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from Pix2Text) (4.67.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from Pix2Text) (2.0.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from Pix2Text) (4.11.0.86)\n",
            "Collecting cnocr>=2.3.0.2 (from cnocr[ort-cpu]>=2.3.0.2->Pix2Text)\n",
            "  Downloading cnocr-2.3.1-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting cnstd>=1.2.4.2 (from Pix2Text)\n",
            "  Downloading cnstd-1.2.5.2-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from Pix2Text) (11.1.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from Pix2Text) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from Pix2Text) (0.21.0+cu124)\n",
            "Requirement already satisfied: transformers>=4.37.0 in /usr/local/lib/python3.11/dist-packages (from Pix2Text) (4.50.3)\n",
            "Collecting optimum[onnxruntime] (from Pix2Text)\n",
            "  Downloading optimum-1.24.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.11/dist-packages (from Pix2Text) (1.25.5)\n",
            "Collecting pyspellchecker (from Pix2Text)\n",
            "  Downloading pyspellchecker-0.8.2-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting doclayout-yolo<0.1 (from Pix2Text)\n",
            "  Downloading doclayout_yolo-0.0.3-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting pytorch-lightning>=2.0.0 (from cnocr>=2.3.0.2->cnocr[ort-cpu]>=2.3.0.2->Pix2Text)\n",
            "  Downloading pytorch_lightning-2.5.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (from cnocr>=2.3.0.2->cnocr[ort-cpu]>=2.3.0.2->Pix2Text) (0.19.9)\n",
            "Collecting torchmetrics (from cnocr>=2.3.0.2->cnocr[ort-cpu]>=2.3.0.2->Pix2Text)\n",
            "  Downloading torchmetrics-1.7.1-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting onnx (from cnocr>=2.3.0.2->cnocr[ort-cpu]>=2.3.0.2->Pix2Text)\n",
            "  Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Collecting rapidocr-onnxruntime<1.4 (from cnocr>=2.3.0.2->cnocr[ort-cpu]>=2.3.0.2->Pix2Text)\n",
            "  Downloading rapidocr_onnxruntime-1.3.25-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting onnxruntime (from cnocr[ort-cpu]>=2.3.0.2->Pix2Text)\n",
            "  Downloading onnxruntime-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from cnstd>=1.2.4.2->Pix2Text) (6.0.2)\n",
            "Collecting unidecode (from cnstd>=1.2.4.2->Pix2Text)\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from cnstd>=1.2.4.2->Pix2Text) (1.14.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from cnstd>=1.2.4.2->Pix2Text) (2.2.2)\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.11/dist-packages (from cnstd>=1.2.4.2->Pix2Text) (2.1.0)\n",
            "Collecting pyclipper (from cnstd>=1.2.4.2->Pix2Text)\n",
            "  Downloading pyclipper-1.3.0.post6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from cnstd>=1.2.4.2->Pix2Text) (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (from cnstd>=1.2.4.2->Pix2Text) (0.13.2)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from cnstd>=1.2.4.2->Pix2Text) (0.30.1)\n",
            "Collecting ultralytics (from cnstd>=1.2.4.2->Pix2Text)\n",
            "  Downloading ultralytics-8.3.107-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from doclayout-yolo<0.1->Pix2Text) (2.32.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from doclayout-yolo<0.1->Pix2Text) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from doclayout-yolo<0.1->Pix2Text) (9.0.0)\n",
            "Collecting thop>=0.1.1 (from doclayout-yolo<0.1->Pix2Text)\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: albumentations>=1.4.11 in /usr/local/lib/python3.11/dist-packages (from doclayout-yolo<0.1->Pix2Text) (2.0.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->Pix2Text) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->Pix2Text) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->Pix2Text) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->Pix2Text) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->Pix2Text) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->Pix2Text)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->Pix2Text)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->Pix2Text)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->Pix2Text)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->Pix2Text)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->Pix2Text)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->Pix2Text)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->Pix2Text)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->Pix2Text)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->Pix2Text) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->Pix2Text) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->Pix2Text) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->Pix2Text)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->Pix2Text) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->Pix2Text) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->Pix2Text) (1.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.37.0->Pix2Text) (24.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.37.0->Pix2Text) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.37.0->Pix2Text) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.37.0->Pix2Text) (0.5.3)\n",
            "Collecting datasets>=1.2.1 (from optimum[onnxruntime]->Pix2Text)\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting evaluate (from optimum[onnxruntime]->Pix2Text)\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: protobuf>=3.20.1 in /usr/local/lib/python3.11/dist-packages (from optimum[onnxruntime]->Pix2Text) (5.29.4)\n",
            "Collecting transformers>=4.37.0 (from Pix2Text)\n",
            "  Downloading transformers-4.48.3-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.11/dist-packages (from albumentations>=1.4.11->doclayout-yolo<0.1->Pix2Text) (2.11.2)\n",
            "Requirement already satisfied: albucore==0.0.23 in /usr/local/lib/python3.11/dist-packages (from albumentations>=1.4.11->doclayout-yolo<0.1->Pix2Text) (0.0.23)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.11/dist-packages (from albumentations>=1.4.11->doclayout-yolo<0.1->Pix2Text) (4.11.0.86)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.23->albumentations>=1.4.11->doclayout-yolo<0.1->Pix2Text) (3.12.3)\n",
            "Requirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.23->albumentations>=1.4.11->doclayout-yolo<0.1->Pix2Text) (6.2.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=1.2.1->optimum[onnxruntime]->Pix2Text) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=1.2.1->optimum[onnxruntime]->Pix2Text)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting xxhash (from datasets>=1.2.1->optimum[onnxruntime]->Pix2Text)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets>=1.2.1->optimum[onnxruntime]->Pix2Text)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec (from torch->Pix2Text)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=1.2.1->optimum[onnxruntime]->Pix2Text) (3.11.15)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->cnstd>=1.2.4.2->Pix2Text) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->cnstd>=1.2.4.2->Pix2Text) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->cnstd>=1.2.4.2->Pix2Text) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->cnstd>=1.2.4.2->Pix2Text) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->cnstd>=1.2.4.2->Pix2Text) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->cnstd>=1.2.4.2->Pix2Text) (2.8.2)\n",
            "Collecting coloredlogs (from onnxruntime->cnocr[ort-cpu]>=2.3.0.2->Pix2Text)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime->cnocr[ort-cpu]>=2.3.0.2->Pix2Text) (25.2.10)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->cnstd>=1.2.4.2->Pix2Text) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->cnstd>=1.2.4.2->Pix2Text) (2025.2)\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch-lightning>=2.0.0->cnocr>=2.3.0.2->cnocr[ort-cpu]>=2.3.0.2->Pix2Text)\n",
            "  Downloading lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: six>=1.15.0 in /usr/local/lib/python3.11/dist-packages (from rapidocr-onnxruntime<1.4->cnocr>=2.3.0.2->cnocr[ort-cpu]>=2.3.0.2->Pix2Text) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->doclayout-yolo<0.1->Pix2Text) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->doclayout-yolo<0.1->Pix2Text) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->doclayout-yolo<0.1->Pix2Text) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->doclayout-yolo<0.1->Pix2Text) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->Pix2Text) (3.0.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics->cnstd>=1.2.4.2->Pix2Text)\n",
            "  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb->cnocr>=2.3.0.2->cnocr[ort-cpu]>=2.3.0.2->Pix2Text) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->cnocr>=2.3.0.2->cnocr[ort-cpu]>=2.3.0.2->Pix2Text) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb->cnocr>=2.3.0.2->cnocr[ort-cpu]>=2.3.0.2->Pix2Text) (4.3.7)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->cnocr>=2.3.0.2->cnocr[ort-cpu]>=2.3.0.2->Pix2Text) (2.25.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb->cnocr>=2.3.0.2->cnocr[ort-cpu]>=2.3.0.2->Pix2Text) (1.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb->cnocr>=2.3.0.2->cnocr[ort-cpu]>=2.3.0.2->Pix2Text) (75.2.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=1.2.1->optimum[onnxruntime]->Pix2Text) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=1.2.1->optimum[onnxruntime]->Pix2Text) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=1.2.1->optimum[onnxruntime]->Pix2Text) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=1.2.1->optimum[onnxruntime]->Pix2Text) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=1.2.1->optimum[onnxruntime]->Pix2Text) (6.3.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=1.2.1->optimum[onnxruntime]->Pix2Text) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=1.2.1->optimum[onnxruntime]->Pix2Text) (1.18.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->cnocr>=2.3.0.2->cnocr[ort-cpu]>=2.3.0.2->Pix2Text) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations>=1.4.11->doclayout-yolo<0.1->Pix2Text) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations>=1.4.11->doclayout-yolo<0.1->Pix2Text) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations>=1.4.11->doclayout-yolo<0.1->Pix2Text) (0.4.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime->cnocr[ort-cpu]>=2.3.0.2->Pix2Text)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->cnocr>=2.3.0.2->cnocr[ort-cpu]>=2.3.0.2->Pix2Text) (5.0.2)\n",
            "Downloading pix2text-1.1.2.3-py3-none-any.whl (215 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.7/215.7 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cnocr-2.3.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.6/224.6 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cnstd-1.2.5.2-py3-none-any.whl (253 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.4/253.4 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading doclayout_yolo-0.0.3-py3-none-any.whl (711 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m711.2/711.2 kB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.48.3-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m90.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyspellchecker-0.8.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_lightning-2.5.1-py3-none-any.whl (822 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.0/823.0 kB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidocr_onnxruntime-1.3.25-py3-none-any.whl (14.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.9/14.9 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyclipper-1.3.0.post6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (969 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m969.6/969.6 kB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Downloading torchmetrics-1.7.1-py3-none-any.whl (961 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m961.5/961.5 kB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading optimum-1.24.0-py3-none-any.whl (433 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m433.6/433.6 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics-8.3.107-py3-none-any.whl (974 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m974.5/974.5 kB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n",
            "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n",
            "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyclipper, xxhash, unidecode, pyspellchecker, onnx, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, humanfriendly, fsspec, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, coloredlogs, onnxruntime, nvidia-cusolver-cu12, transformers, rapidocr-onnxruntime, datasets, ultralytics-thop, torchmetrics, thop, optimum, evaluate, ultralytics, pytorch-lightning, doclayout-yolo, cnstd, cnocr, Pix2Text\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.50.3\n",
            "    Uninstalling transformers-4.50.3:\n",
            "      Successfully uninstalled transformers-4.50.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Pix2Text-1.1.2.3 cnocr-2.3.1 cnstd-1.2.5.2 coloredlogs-15.0.1 datasets-3.5.0 dill-0.3.8 doclayout-yolo-0.0.3 evaluate-0.4.3 fsspec-2024.12.0 humanfriendly-10.0 lightning-utilities-0.14.3 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 onnx-1.17.0 onnxruntime-1.21.0 optimum-1.24.0 pyclipper-1.3.0.post6 pyspellchecker-0.8.2 pytorch-lightning-2.5.1 rapidocr-onnxruntime-1.3.25 thop-0.1.1.post2209072238 torchmetrics-1.7.1 transformers-4.48.3 ultralytics-8.3.107 ultralytics-thop-2.0.14 unidecode-1.3.8 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install PyMuPDF\n",
        "!pip install Pix2Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9f4YrfAuVPK",
        "outputId": "583cd999-8391-4022-d89b-284b211d3b11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import fitz  # PyMuPDF\n",
        "from pix2text import Pix2Text\n",
        "import google.generativeai as genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XZ6Zjf-JEa0",
        "outputId": "8073e40d-dff0-4325-cd41-5e7b1cd7df13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZwZVKwgZYMQK"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = \"/content/drive/My Drive/BDS_Topic_Clustering/final_jsons\"\n",
        "OUTPUT_DIR = \"/content/drive/My Drive/BDS_Topic_Clustering/fresh_output_embeddings\"\n",
        "NEW_FILES_DIR = \"/content/drive/My Drive/BDS_Topic_Clustering/new_pdf_files\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUynGR8_YvMZ"
      },
      "source": [
        "# **1. Convert PDFs to Markdowns**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "9ea661c9e74d4cd1b9c6db2b7af8a3ec",
            "acac42bf9a58430f9ded1727c3fe3692",
            "7674bfca28244dbba8d086b8f060ee3a",
            "9c016d00468944a78fc29406d96d080f",
            "c238b39654004f03b33d9d2e1e2e3132",
            "27a1d8449c0a4e22bb4b86bb0eefed1f",
            "c64ec0f9ebe746b3bc08c24b49f759fc",
            "a5d202508e9041b8a6b018e64aaef8f1",
            "3003a5cf84f048ce97985e2cdb6c73b2",
            "1934c82949ad45249c1467ef04f494cd",
            "49a841099edc437f97a645854cb066a0"
          ]
        },
        "collapsed": true,
        "id": "O_sbLxmysUrQ",
        "outputId": "e19f4ef8-69b0-4960-93c9-68ac83c7ed52"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:cnocr.recognizer:no onnx file is found in /root/.cnocr/2.3/densenet_lite_136-gru\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9ea661c9e74d4cd1b9c6db2b7af8a3ec",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "densenet_lite_136-gru-onnx.zip:   0%|          | 0.00/11.6M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:cnstd.ppocr.rapid_detector:can not find model file /root/.cnstd/1.2/ppocr/ch_PP-OCRv4_det/ch_PP-OCRv4_det_infer.onnx\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 1024x736 1 title, 10 plain texts, 6 abandons, 1 table, 1 table_caption, 125.5ms\n",
            "Speed: 33.7ms preprocess, 125.5ms inference, 335.1ms postprocess per image at shape (1, 3, 1024, 736)\n",
            "Loading /root/.pix2text/1.1/mfd-onnx/mfd-v20240618.onnx for ONNX Runtime inference...\n",
            "Using ONNX Runtime CPUExecutionProvider\n",
            "\n",
            "WARNING ⚠️ imgsz=[198, 1280] must be multiple of max stride 32, updating to [224, 1280]\n",
            "0: 224x1280 (no detections), 235.6ms\n",
            "Speed: 2.8ms preprocess, 235.6ms inference, 8.2ms postprocess per image at shape (1, 3, 224, 1280)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING ⚠️ imgsz=[583, 1280] must be multiple of max stride 32, updating to [608, 1280]\n",
            "0: 608x1280 1 isolated, 1120.3ms\n",
            "Speed: 7.1ms preprocess, 1120.3ms inference, 12.9ms postprocess per image at shape (1, 3, 608, 1280)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:01<00:00,  1.84s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING ⚠️ imgsz=[275, 1280] must be multiple of max stride 32, updating to [288, 1280]\n",
            "0: 288x1280 (no detections), 573.6ms\n",
            "Speed: 3.8ms preprocess, 573.6ms inference, 1.0ms postprocess per image at shape (1, 3, 288, 1280)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING ⚠️ imgsz=[767, 1280] must be multiple of max stride 32, updating to [768, 1280]\n",
            "0: 768x1280 1 embedding, 1616.4ms\n",
            "Speed: 18.2ms preprocess, 1616.4ms inference, 1.7ms postprocess per image at shape (1, 3, 768, 1280)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING ⚠️ imgsz=[687, 1280] must be multiple of max stride 32, updating to [704, 1280]\n",
            "0: 704x1280 (no detections), 1321.2ms\n",
            "Speed: 9.1ms preprocess, 1321.2ms inference, 4.0ms postprocess per image at shape (1, 3, 704, 1280)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 640x1280 (no detections), 1191.9ms\n",
            "Speed: 21.4ms preprocess, 1191.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 1280)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING ⚠️ imgsz=[656, 1280] must be multiple of max stride 32, updating to [672, 1280]\n",
            "0: 672x1280 (no detections), 1457.2ms\n",
            "Speed: 9.0ms preprocess, 1457.2ms inference, 0.9ms postprocess per image at shape (1, 3, 672, 1280)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING ⚠️ imgsz=[375, 1280] must be multiple of max stride 32, updating to [384, 1280]\n",
            "0: 384x1280 (no detections), 1074.4ms\n",
            "Speed: 10.6ms preprocess, 1074.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 1280)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING ⚠️ imgsz=[543, 1280] must be multiple of max stride 32, updating to [544, 1280]\n",
            "0: 544x1280 (no detections), 959.8ms\n",
            "Speed: 30.9ms preprocess, 959.8ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 1280)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING ⚠️ imgsz=[373, 1280] must be multiple of max stride 32, updating to [384, 1280]\n",
            "0: 384x1280 (no detections), 704.7ms\n",
            "Speed: 5.2ms preprocess, 704.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 1280)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING ⚠️ imgsz=[407, 1280] must be multiple of max stride 32, updating to [416, 1280]\n",
            "0: 416x1280 (no detections), 412.3ms\n",
            "Speed: 3.8ms preprocess, 412.3ms inference, 0.6ms postprocess per image at shape (1, 3, 416, 1280)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING ⚠️ imgsz=[1811, 1280] must be multiple of max stride 32, updating to [1824, 1280]\n",
            "0: 1824x1280 (no detections), 2441.4ms\n",
            "Speed: 17.4ms preprocess, 2441.4ms inference, 1.4ms postprocess per image at shape (1, 3, 1824, 1280)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved Markdown to /content/drive/My Drive/BDS_Topic_Clustering/new_pdfs_mds/4788/page_1.md\n",
            "Completed processing 4788.pdf\n"
          ]
        }
      ],
      "source": [
        "# code from pixImple.py\n",
        "\n",
        "def run_pix2text(img_fp, page_number):\n",
        "    p2t = Pix2Text.from_config(device=\"cpu\")\n",
        "    kwargs = {\n",
        "        \"resized_shape\": 1280,\n",
        "        \"mfr_batch_size\": 1,\n",
        "        \"embed_sep\": (\" $\", \"$ \"),\n",
        "        \"isolated_sep\": (\"$$\", \"$$\"),\n",
        "        \"line_sep\": \"\\n\",\n",
        "        \"auto_line_break\": True,\n",
        "        \"det_text_bbox_max_width_expand_ratio\": 0.3,\n",
        "        \"det_text_bbox_max_height_expand_ratio\": 0.2,\n",
        "        \"embed_ratio_threshold\": 0.6,\n",
        "        \"table_as_image\": True,\n",
        "        \"formula_rec_kwargs\": {}\n",
        "    }\n",
        "    try:\n",
        "        page = p2t.recognize_page(img_fp, page_number=page_number, **kwargs)\n",
        "        return page\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing page {page_number}: {e}\")\n",
        "        return None\n",
        "\n",
        "def save_pix2text_output(page, page_number, output_page_dir):\n",
        "    os.makedirs(output_page_dir, exist_ok=True)\n",
        "    output_md_path = os.path.join(output_page_dir, f'page_{page_number}.md')\n",
        "    try:\n",
        "        page.to_markdown(output_md_path)\n",
        "        print(f\"Saved Markdown to {output_md_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving Markdown for page {page_number}: {e}\")\n",
        "\n",
        "    if hasattr(page, 'images') and page.images:\n",
        "        for img_name, img_data in page.images.items():\n",
        "            output_img_path = os.path.join(output_page_dir, f'{page_number}_img_{img_name}')\n",
        "            try:\n",
        "                with open(output_img_path, 'wb') as img_file:\n",
        "                    img_file.write(img_data)\n",
        "                print(f\"Saved image to {output_img_path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error saving image {img_name} for page {page_number}: {e}\")\n",
        "\n",
        "# modified from pixImple to process PDFs by each page into markdown files\n",
        "def process_pdfs(input_dir, output_dir):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    pdf_files = [f for f in os.listdir(input_dir) if f.endswith('.pdf')]\n",
        "\n",
        "    for pdf_file in pdf_files:\n",
        "        pdf_path = os.path.join(input_dir, pdf_file)\n",
        "        pdf_output_dir = os.path.join(output_dir, os.path.splitext(pdf_file)[0])\n",
        "        os.makedirs(pdf_output_dir, exist_ok=True)\n",
        "\n",
        "        doc = fitz.open(pdf_path)\n",
        "        for page_number in range(len(doc)):\n",
        "            page = doc.load_page(page_number)\n",
        "            img_fp = os.path.join(pdf_output_dir, f'page_{page_number + 1}.png')\n",
        "            pix = page.get_pixmap()\n",
        "            pix.save(img_fp)\n",
        "\n",
        "            result = run_pix2text(img_fp, page_number + 1)\n",
        "            if result:\n",
        "                save_pix2text_output(result, page_number + 1, pdf_output_dir)\n",
        "\n",
        "        doc.close()\n",
        "        print(f\"Completed processing {pdf_file}\")\n",
        "\n",
        "# Process pdfs into output folder\n",
        "input_dir = NEW_FILES_DIR\n",
        "output_dir = \"/content/drive/My Drive/BDS_Topic_Clustering/new_pdfs_mds\"\n",
        "process_pdfs(input_dir, output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXsqiKXKY9Nn"
      },
      "source": [
        "# **2. Combine Markdowns into One**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rk-LF01VuY-A"
      },
      "outputs": [],
      "source": [
        "# function to combine all markdowns within each pdf folder to output single markdown\n",
        "def combine_output_markdowns(output_dir):\n",
        "    for folder in os.listdir(output_dir):\n",
        "        folder_path = os.path.join(output_dir, folder)\n",
        "        if os.path.isdir(folder_path):\n",
        "            combined_md_path = os.path.join(folder_path, f'{folder}_combined.md')\n",
        "            with open(combined_md_path, 'w', encoding='utf-8') as combined_file:\n",
        "                markdown_found = False\n",
        "                # Traverse subdirectories to find output.md\n",
        "                for subdir in os.listdir(folder_path):\n",
        "                    subdir_path = os.path.join(folder_path, subdir)\n",
        "                    if os.path.isdir(subdir_path):\n",
        "                        output_md_path = os.path.join(subdir_path, 'output.md')\n",
        "                        if os.path.isfile(output_md_path):\n",
        "                            with open(output_md_path, 'r', encoding='utf-8') as md_file:\n",
        "                                content = md_file.read().strip()\n",
        "                                if content:\n",
        "                                    combined_file.write(f'# {subdir}\\n\\n')  # Subdir name as header\n",
        "                                    combined_file.write(content + '\\n\\n')\n",
        "                                    markdown_found = True\n",
        "\n",
        "                if not markdown_found:\n",
        "                    print(f\"No output.md files found in {folder_path}, combined file will be empty.\")\n",
        "                else:\n",
        "                    print(f\"Combined markdown saved to {combined_md_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpz6Eh6puZPV",
        "outputId": "c6fb7631-b9d6-4e9c-b770-15edf1ba9071"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Combined markdown saved to /content/drive/My Drive/BDS_Topic_Clustering/new_pdfs_mds/4788/4788_combined.md\n"
          ]
        }
      ],
      "source": [
        "# Combine markdowns\n",
        "output_dir = \"/content/drive/My Drive/BDS_Topic_Clustering/new_pdfs_mds\"\n",
        "combine_output_markdowns(output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnTyphL9ZBAm"
      },
      "source": [
        "# **3. Build JSON from Combined Markdown**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPSpgCLyYk8b"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKB--cGez1FA"
      },
      "outputs": [],
      "source": [
        "# Set your API key\n",
        "genai.configure(api_key = userdata.get('GEMINI_KEY'))\n",
        "# Load the JSON schema\n",
        "with open(\"/content/drive/My Drive/BDS_Topic_Clustering/json_schema.txt\", \"r\") as file:\n",
        "    json_schema = file.read()\n",
        "\n",
        "# Function to process a single Markdown file\n",
        "def process_markdown_file(md_file_path):\n",
        "    with open(md_file_path, \"r\") as file:\n",
        "        markdown_content = file.read()\n",
        "\n",
        "    # Prepare the prompt\n",
        "    prompt = f\"\"\"\n",
        "    Based on the following JSON schema, extract the information from the provided Markdown content and generate a JSON output:\n",
        "\n",
        "    JSON Schema:\n",
        "    {json_schema}\n",
        "\n",
        "    Markdown Content:\n",
        "    {markdown_content}\n",
        "\n",
        "    Please produce a JSON output following the schema exactly.\n",
        "    \"\"\"\n",
        "\n",
        "    # Call the Gemini API\n",
        "    # model = genai.GenerativeModel('gemini-2.0-pro-exp') #or gemini-pro-vision if you need it.\n",
        "    model = genai.GenerativeModel('gemini-2.0-flash') #or gemini-pro-vision if you need it.\n",
        "    response = model.generate_content(prompt)\n",
        "\n",
        "    # Get the generated JSON\n",
        "    generated_json = response.text\n",
        "\n",
        "    # Save JSON file in the same folder as the Markdown file\n",
        "    output_file_path = os.path.splitext(md_file_path)[0] + \".json\"\n",
        "    with open(output_file_path, \"w\") as file:\n",
        "        file.write(generated_json)\n",
        "\n",
        "    print(f\"JSON output saved to {output_file_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "LYSeq1wn0BtF",
        "outputId": "d450e327-b960-4468-af8f-dfa55986ed35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing /content/drive/My Drive/BDS_Topic_Clustering/new_pdfs_mds/4788/4788_combined.md...\n",
            "JSON output saved to /content/drive/My Drive/BDS_Topic_Clustering/new_pdfs_mds/4788/4788_combined.json\n",
            "Processing complete.\n"
          ]
        }
      ],
      "source": [
        "# Traverse multiple folders and process each Markdown file\n",
        "root_dir = \"/content/drive/My Drive/BDS_Topic_Clustering/new_pdfs_mds\"\n",
        "target_subdirs = [\"4788\"]\n",
        "\n",
        "for subdir in target_subdirs:\n",
        "    subdir_path = os.path.join(root_dir, subdir)\n",
        "\n",
        "    for file in os.listdir(subdir_path):\n",
        "        if file.endswith(\".md\") and os.path.isfile(os.path.join(subdir_path, file)):\n",
        "            md_file_path = os.path.join(subdir_path, file)\n",
        "\n",
        "            # Check if the .md file is directly in the target subdirectory (not in a nested subdirectory)\n",
        "            if os.path.dirname(md_file_path) == subdir_path:\n",
        "                print(f\"Processing {md_file_path}...\")\n",
        "                process_markdown_file(md_file_path)\n",
        "\n",
        "print(\"Processing complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qgBHDnIQmrd"
      },
      "source": [
        "# **CREATING EMBEDDINGS AND CLUSTERS FROM OLD FILES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WCtdvjeYmxc",
        "outputId": "7bd0f0a5-1a5c-43f4-b68c-52aed5e3dff8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.48.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.14.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.30.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.12.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n",
            "Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.10.0\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.70.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.4)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.24.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.164.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.4)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.11.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.13.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.69.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.4.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install nltk\n",
        "!pip install tqdm\n",
        "!pip install faiss-cpu sentence-transformers\n",
        "!pip install openai\n",
        "!pip install google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GayIIUcCaMgV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "import google.generativeai as genai\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import nltk\n",
        "import faiss\n",
        "import re\n",
        "from collections import Counter\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from tqdm import tqdm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics.pairwise import euclidean_distances"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pt4g7EigGbZ"
      },
      "source": [
        "Link to All folders - https://drive.google.com/drive/folders/19jKtMboy7-_yJW4cfWFLAUzrvt-xJRzy?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6BnOpLtYxOR",
        "outputId": "1aa50077-09b2-43c5-f224-00dd31829077"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "9GiYIJU6ZhyF"
      },
      "outputs": [],
      "source": [
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Initialize components\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b304CNwdaUdX"
      },
      "source": [
        "# **1. Load JSON files and extract text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDkFX08RaQal"
      },
      "outputs": [],
      "source": [
        "def load_json_files(data_dir):\n",
        "    #Load JSON files and extract relevant text fields, handling missing values.\n",
        "    all_texts = []\n",
        "    file_ids = []\n",
        "\n",
        "    for file in os.listdir(data_dir):\n",
        "        if file.endswith(\".json\"):\n",
        "            file_path = os.path.join(data_dir, file)\n",
        "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            # Extract text, handle missing values\n",
        "            title = data[\"metadata\"].get(\"title\", \"\") or \"\"\n",
        "            abstract = \" \".join([para.get(\"text\", \"\") for para in data[\"metadata\"].get(\"abstract\", []) if para.get(\"text\")])\n",
        "            body_text = \" \".join([para.get(\"text\", \"\") for para in data[\"metadata\"].get(\"body_text\", []) if para.get(\"text\")])\n",
        "\n",
        "            # Combine all extracted text\n",
        "            combined_text = f\"{title} {abstract} {body_text}\".strip()\n",
        "\n",
        "            if combined_text:  # Ensure non-empty entries\n",
        "                file_ids.append(file.split(\".json\")[0])\n",
        "                all_texts.append(combined_text)\n",
        "\n",
        "    return file_ids, all_texts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GD9QngctdfD_"
      },
      "source": [
        "# **2. Preprocess Text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "zIHNNh3ldfsv"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    #Lowercase, tokenize, remove stopwords, and lemmatize.\n",
        "    words = word_tokenize(text.lower())  # Lowercasing & tokenization\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word.isalnum() and word not in stop_words]\n",
        "    return \" \".join(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELkjhA49dyEH"
      },
      "source": [
        "# **3. Generate TF-IDF Embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQiv9G_1dyoI"
      },
      "outputs": [],
      "source": [
        "def generate_tfidf_embeddings(texts):\n",
        "    #Compute TF-IDF embeddings.\n",
        "    vectorizer = TfidfVectorizer(max_features=5000)  # Limit features for efficiency\n",
        "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
        "    return tfidf_matrix.toarray(), vectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akwwEgtTffxp"
      },
      "source": [
        "# **4. Save embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fCCAJvGfgSP"
      },
      "outputs": [],
      "source": [
        "def save_embeddings(file_ids, tfidf_embeddings):\n",
        "    #Save embeddings as .npy files and CSV summaries.\n",
        "    np.save(os.path.join(OUTPUT_DIR, \"tfidf_embeddings.npy\"), tfidf_embeddings)\n",
        "\n",
        "    # Save file mappings for reference\n",
        "    df = pd.DataFrame({\"file_id\": file_ids})\n",
        "    df.to_csv(os.path.join(OUTPUT_DIR, \"file_mappings.csv\"), index=False)\n",
        "\n",
        "    print(\"Embeddings saved successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uRL3d_Sg4NJ"
      },
      "source": [
        "# **5. Run the pipeline**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRWOaWwPg4kH",
        "outputId": "6a5a9596-f09f-4c0c-913f-e0058ff3742e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading JSON files...\n",
            "Preprocessing text...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Preprocessing: 100%|██████████| 79/79 [00:04<00:00, 17.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating TF-IDF embeddings...\n",
            "Saving embeddings...\n",
            "Embeddings saved successfully!\n",
            "All done!\n"
          ]
        }
      ],
      "source": [
        "print(\"Loading JSON files...\")\n",
        "file_ids, raw_texts = load_json_files(DATA_DIR)\n",
        "\n",
        "print(\"Preprocessing text...\")\n",
        "processed_texts = [preprocess_text(text) for text in tqdm(raw_texts, desc=\"Preprocessing\")]\n",
        "\n",
        "print(\"Generating TF-IDF embeddings...\")\n",
        "tfidf_embeddings, vectorizer = generate_tfidf_embeddings(processed_texts)\n",
        "\n",
        "print(\"Saving embeddings...\")\n",
        "save_embeddings(file_ids, tfidf_embeddings)\n",
        "\n",
        "print(\"All done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kz70T-vakU-7"
      },
      "source": [
        "# **6. Saving the TF-IDF vectorizer for future use**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MmU8TB4kUtp"
      },
      "outputs": [],
      "source": [
        "# Save vectorizer\n",
        "import pickle\n",
        "with open(os.path.join(OUTPUT_DIR, \"tfidf_vectorizer.pkl\"), \"wb\") as f:\n",
        "    pickle.dump(vectorizer, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWDECxk3mdXG"
      },
      "source": [
        "# **7. Checking shapes of the embeddings (to ensure all files were processed)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEJTOODAnOeO",
        "outputId": "7117b883-1d97-489f-cd2f-3e87b61e7c85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of JSONs processed: 79\n",
            "TF-IDF Embeddings Shape: (79, 5000)\n"
          ]
        }
      ],
      "source": [
        "# Load file mappings\n",
        "df = pd.read_csv(\"/content/drive/My Drive/BDS_Topic_Clustering/output_embeddings/file_mappings.csv\")\n",
        "\n",
        "# Load embeddings\n",
        "tfidf_embeddings = np.load(\"/content/drive/My Drive/BDS_Topic_Clustering/output_embeddings/tfidf_embeddings.npy\")\n",
        "\n",
        "# Check counts\n",
        "print(f\"Number of JSONs processed: {len(df)}\")\n",
        "print(f\"TF-IDF Embeddings Shape: {tfidf_embeddings.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5Di940swID7"
      },
      "source": [
        "# **8. Class-based TF-IDF and Topic Extraction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-wUDNTfwHTl",
        "outputId": "cc0875a7-d176-4e23-fe96-00de8ecf1a52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF-IDF Embeddings Shape: (79, 5000)\n",
            "Number of documents: 79\n",
            "Loading document texts...\n",
            "Extracting top keywords...\n",
            "Finding representative documents...\n",
            "\n",
            "**Cluster Summary**\n",
            "\n",
            " Cluster 0:\n",
            "Representative Documents:\n",
            "   - Search for Multibody Nuclear Reactions in Metal Deuteride Induced with Ion Beam and Electrolysis Methods - We report here the experimental results suggesting the occurrence of multibody nuclear reactions in metal deuterides under ion-beam irradiation and electrolysis. A meaningful increase of\n",
            "helium-4 was ...\n",
            "\n",
            "   - PRODUCTION OF HIGH ENERGY CHARGED PARTICLES DURING DEUTERON IMPLANTATION OF TITANIUM DEUTERIDES - Implantation experiments using 300-keV deuteron beams are performed to study the 3-body reaction in metal deuterides with full use of in situ analyses of the target. The AE-E telescope and the angular...\n",
            "\n",
            "Top Keywords: reaction, mev, spectrum, energy, target, deuteron\n",
            "\n",
            " Cluster 1:\n",
            "Representative Documents:\n",
            "   - PROGRESS TOWARDS REPLICATION - ...\n",
            "\n",
            "   - Input to Theory from Experiment in the Fleischmann-Pons Effect - Excess heat in the Flesichmann-Pons effect constitutes a new physical effect unlike other physical processes with which we are familiar. Many groups have proposed theoretical mechanisms to account for...\n",
            "\n",
            "   - DOES COLD NUCLEAR FUSION EXIST? - Results ef the investication of cold nucicar fusion on palladium are reporied both for electrolysis of heavy water D:O and a muture D,O+H.O ( \\\\\\l and for palladium uturaled with deuienum gaa. The eau...\n",
            "\n",
            "Top Keywords: pd, experiment, heat, neutron, energy, palladium\n",
            "\n",
            " Cluster 2:\n",
            "Representative Documents:\n",
            "   - Scarch for cold fusion using x-ray detection - A search for x rays excited by charged-particle fusion products in a Pt-Pd electrolytic cell with Li-D:O electrolyte is reported. Electrolytic londing of the palladium cathode was found to be 0,S deut...\n",
            "\n",
            "   - Excess Heat Production in Electrolysis of Potassium Carbonate Solution with Nickel Electrodes - ...\n",
            "\n",
            "   - Electrochemical Calorimetric Studies on Water and Deuterium Oxide Electrolysis. - This talk will focus on our preliminary results on calorimetric studies on water and deuterium oxide electrolysis using Pt or Pd cathodes with LiOH(D) and NaF as the supporting electrolytes. In some c...\n",
            "\n",
            "Top Keywords: cell, solution, temperature, water, electrolyte, reaction\n",
            "\n",
            " Cluster 3:\n",
            "Representative Documents:\n",
            "   - Molecular D2 Near Vacancies in PdD and Related Problems - The electron density is too high in PlD for molecular I \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\mathbf{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\Delta} to form, so we consider the problem of E \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\cdot farmation in a monowacancy, and related...\n",
            "\n",
            "   - NUMERICAL CALCULATIONS OF COLD FUSION RATES IN METAL DEUTERIDES - the electron and deuterons (protons) motion is described by the equations of motion...\n",
            "\n",
            "   - Tunneling Effect Enhanced by Lattice Screening as Main Cold Fusion Mechanism: An Brief Theoretical Overview - In this paper are illustratedthe main features of tuneling trveling between two deuterons withn alatice. Considering the screening effect due lattice electrons, we conpare the d d fusion rate evaluate...\n",
            "\n",
            "Top Keywords: energy, electron, fusion, deuteron, nucleus, pd\n",
            "\n",
            " Cluster 4:\n",
            "Representative Documents:\n",
            "   - The War Against Cold Fusion What's really behind it? - ...\n",
            "\n",
            "   - Lessons from cold fusion, 30 years on - ...\n",
            "\n",
            "   - BOOK REVIEW - by Jed Rothwell A Dialogue on Chemically Induced Nuclear Effects-- - ...\n",
            "\n",
            "Top Keywords: patent, claim, fusion, cold, may, heat\n",
            "\n",
            "Cluster summary saved as cluster_summary.csv\n"
          ]
        }
      ],
      "source": [
        "tfidf_embeddings = np.load(os.path.join(OUTPUT_DIR, \"tfidf_embeddings.npy\"))\n",
        "df_files = pd.read_csv(os.path.join(OUTPUT_DIR, \"file_mappings.csv\"))  # File mappings\n",
        "\n",
        "print(f\"TF-IDF Embeddings Shape: {tfidf_embeddings.shape}\")\n",
        "print(f\"Number of documents: {len(df_files)}\")\n",
        "\n",
        "# Cluster Using K-Means\n",
        "NUM_CLUSTERS = 5\n",
        "kmeans = KMeans(n_clusters=NUM_CLUSTERS, random_state=42)\n",
        "df_files[\"cluster\"] = kmeans.fit_predict(tfidf_embeddings)\n",
        "\n",
        "# Saving K-Means model for future use\n",
        "with open(os.path.join(OUTPUT_DIR, \"kmeans_model.pkl\"), \"wb\") as f:\n",
        "    pickle.dump(kmeans, f)\n",
        "\n",
        "# Load JSONs and Extract Texts (Titles & Abstracts)\n",
        "def load_texts(data_dir):\n",
        "    texts = {}\n",
        "    doc_titles = {}\n",
        "    for file in os.listdir(data_dir):\n",
        "        if file.endswith(\".json\"):\n",
        "            file_id = file.replace(\".json\", \"\")  # Ensure file ID is a string\n",
        "            with open(os.path.join(data_dir, file), \"r\", encoding=\"utf-8\") as f:\n",
        "                data = json.load(f)\n",
        "            title = data[\"metadata\"].get(\"title\", \"\") or \"Untitled Document\"\n",
        "            abstract = \" \".join([para.get(\"text\", \"\") for para in data[\"metadata\"].get(\"abstract\", []) if para.get(\"text\")])\n",
        "            body_text = \" \".join([para.get(\"text\", \"\") for para in data[\"metadata\"].get(\"body_text\", []) if para.get(\"text\")])\n",
        "            texts[file_id] = f\"{title} {abstract} {body_text}\".strip()\n",
        "            doc_titles[file_id] = f\"{title} - {abstract[:200]}...\"\n",
        "    return texts, doc_titles\n",
        "\n",
        "print(\"Loading document texts...\")\n",
        "doc_texts, doc_titles = load_texts(DATA_DIR)\n",
        "\n",
        "# Convert file_id in df_files to string\n",
        "df_files[\"file_id\"] = df_files[\"file_id\"].astype(str)\n",
        "\n",
        "# Merge texts within each cluster, ensuring no empty entries\n",
        "cluster_texts = []\n",
        "for i in range(NUM_CLUSTERS):\n",
        "    texts_in_cluster = [doc_texts[file_id] for file_id in df_files[df_files[\"cluster\"] == i][\"file_id\"] if file_id in doc_texts]\n",
        "    merged_text = \" \".join(texts_in_cluster).strip()\n",
        "\n",
        "    # Apply preprocessing to remove stopwords from cluster text\n",
        "    if merged_text:\n",
        "        cluster_texts.append(preprocess_text(merged_text))\n",
        "    else:\n",
        "        cluster_texts.append(\"empty cluster\")\n",
        "\n",
        "# Compute TF-IDF for clusters\n",
        "vectorizer = TfidfVectorizer(max_features=1000)\n",
        "cluster_tfidf = vectorizer.fit_transform(cluster_texts)\n",
        "\n",
        "# Get feature names (words)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Extract Top 6 Keywords Per Cluster\n",
        "print(\"Extracting top keywords...\")\n",
        "top_keywords = {}\n",
        "for i in range(NUM_CLUSTERS):\n",
        "    sorted_indices = cluster_tfidf[i].toarray().argsort()[0][-6:][::-1]  # Get top 6 keywords\n",
        "    top_keywords[i] = [feature_names[idx] for idx in sorted_indices]\n",
        "\n",
        "# Find Representative Documents (Titles & Abstracts)\n",
        "print(\"Finding representative documents...\")\n",
        "rep_docs = {}\n",
        "for i in range(NUM_CLUSTERS):\n",
        "    cluster_indices = df_files[df_files[\"cluster\"] == i].index\n",
        "    cluster_embeddings = tfidf_embeddings[cluster_indices]\n",
        "    centroid = kmeans.cluster_centers_[i].reshape(1, -1)\n",
        "\n",
        "    # Compute distances to centroid\n",
        "    distances = euclidean_distances(cluster_embeddings, centroid).flatten()\n",
        "\n",
        "    # Get indices of top 3 closest documents\n",
        "    top_doc_indices = cluster_indices[np.argsort(distances)[:3]]\n",
        "    rep_docs[i] = [doc_titles[df_files.loc[idx, \"file_id\"]] for idx in top_doc_indices]\n",
        "\n",
        "# Results\n",
        "print(\"\\n**Cluster Summary**\")\n",
        "for cluster_id in range(NUM_CLUSTERS):\n",
        "    print(f\"\\n Cluster {cluster_id}:\")\n",
        "    print(\"Representative Documents:\")\n",
        "    for doc in rep_docs[cluster_id]:\n",
        "        print(f\"   - {doc}\\n\")\n",
        "    print(f\"Top Keywords: {', '.join(top_keywords[cluster_id])}\")\n",
        "\n",
        "# Save results to CSV\n",
        "df_summary = pd.DataFrame({\n",
        "    \"Cluster\": list(range(NUM_CLUSTERS)),\n",
        "    \"Representative Documents\": [\"\\n\".join(rep_docs[i]) for i in range(NUM_CLUSTERS)],\n",
        "    \"Top Keywords\": [\", \".join(top_keywords[i]) for i in range(NUM_CLUSTERS)]\n",
        "})\n",
        "df_summary.to_csv(os.path.join(OUTPUT_DIR, \"cluster_summary.csv\"), index=False)\n",
        "\n",
        "print(\"\\nCluster summary saved as cluster_summary.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkPrWb7HJdLj"
      },
      "source": [
        "# **9. Improving Topic Representations using Gemini**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "ziNBXEJZJPVY",
        "outputId": "2865ccad-6666-4aa4-af42-13cf8519b2b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Cluster 0: Deuteron-Induced Nuclear Reactions in Metal Deuterides\n",
            "\n",
            "\n",
            " Cluster 1: Cold Fusion/Low Energy Nuclear Reactions (LENR) in Palladium\n",
            "\n",
            "\n",
            " Cluster 2: Electrolytic Cell Reactions and Excess Heat:  Studies on electrolysis with various electrolytes (e.g., LiOD, LiOH, NaF, K2CO3) and electrodes (e.g., Pt, Pd, Ni) focusing on excess heat production and potential fusion reactions.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 354.39ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error generating topic for: ['energy', 'electron', 'fusion', 'deuteron', 'nucleus', 'pd'] → 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
            "\n",
            " Cluster 3: Error generating topic\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 328.72ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error generating topic for: ['patent', 'claim', 'fusion', 'cold', 'may', 'heat'] → 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
            "\n",
            " Cluster 4: Error generating topic\n",
            "\n",
            "\n",
            " Refined topics saved as refined_cluster_summary.csv\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "SUMMARY_FILE = os.path.join(OUTPUT_DIR, \"cluster_summary.csv\")\n",
        "\n",
        "# Load existing cluster summaries\n",
        "df_summary = pd.read_csv(SUMMARY_FILE)\n",
        "\n",
        "genai.configure(api_key = userdata.get('NEW_GEMINI_KEY'))\n",
        "\n",
        "gemini_model = genai.GenerativeModel(\"gemini-1.5-pro\")\n",
        "\n",
        "def generate_topic_description_gemini(keywords, representative_docs):\n",
        "\n",
        "    prompt = (\n",
        "        f\"Given the following information about documents (research papers) in a cluster:\\n\"\n",
        "        f\"Keywords: {', '.join(keywords)}\\n\"\n",
        "        f\"Representative Documents: {representative_docs}\\n\"\n",
        "        f\"Please provide a meaningful and concise topic description for this cluster.\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = gemini_model.generate_content(prompt)\n",
        "\n",
        "        # Extract the response\n",
        "        topic_description = response.text.strip()\n",
        "        return topic_description\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating topic for: {keywords} → {e}\")\n",
        "        return \"Error generating topic\"\n",
        "\n",
        "# Generate & Save Refined Topics\n",
        "for index, row in df_summary.iterrows():\n",
        "    time.sleep(10)\n",
        "    refined_topic = generate_topic_description_gemini(row[\"Top Keywords\"].split(\", \"), row[\"Representative Documents\"])\n",
        "\n",
        "    print(f\"\\n Cluster {row['Cluster']}: {refined_topic}\\n\")\n",
        "\n",
        "    df_summary.at[index, \"Refined Topic\"] = refined_topic\n",
        "\n",
        "# Save updated results\n",
        "df_summary.to_csv(os.path.join(OUTPUT_DIR, \"refined_cluster_summary.csv\"), index=False)\n",
        "\n",
        "print(\"\\n Refined topics saved as refined_cluster_summary.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "WHvcmxv38eMb",
        "outputId": "5d3c9be7-8fa5-47d2-913f-c93848f731ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "⏳ Retrying Cluster 3...\n",
            " Cluster 3 updated: **Cold Fusion in Metal Deuterides: Theoretical and Computational Studies**\n",
            "\n",
            "\n",
            "⏳ Retrying Cluster 4...\n",
            " Cluster 4 updated: Cold Fusion Controversy and Research\n",
            "\n",
            "\n",
            " All updates saved to refined_cluster_summary.csv\n"
          ]
        }
      ],
      "source": [
        "# Load existing file\n",
        "df_summary = pd.read_csv(os.path.join(OUTPUT_DIR, \"refined_cluster_summary.csv\"))\n",
        "\n",
        "# Loop over clusters with errors (due to time limits)\n",
        "for index, row in df_summary.iterrows():\n",
        "    if str(row[\"Refined Topic\"]).startswith(\"Error\"):\n",
        "        print(f\"\\n⏳ Retrying Cluster {row['Cluster']}...\")\n",
        "        time.sleep(10)  # Wait to avoid rate limit exhaustion\n",
        "\n",
        "        keywords = [kw.strip() for kw in row[\"Top Keywords\"].split(\",\")]\n",
        "        topic = generate_topic_description_gemini(keywords, row[\"Representative Documents\"])\n",
        "\n",
        "        df_summary.at[index, \"Refined Topic\"] = topic\n",
        "        print(f\" Cluster {row['Cluster']} updated: {topic}\\n\")\n",
        "\n",
        "# Save back to the same file\n",
        "df_summary.to_csv(os.path.join(OUTPUT_DIR, \"refined_cluster_summary.csv\"), index=False)\n",
        "print(\"\\n All updates saved to refined_cluster_summary.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHcCyniYSKFz"
      },
      "source": [
        "Input Prompt:\n",
        "\n",
        "\"Given the following information about documents (research papers) in a cluster:\n",
        "        Keywords: {top 6 keywords}\n",
        "        Representative Documents: {closest 3 representative docs},\n",
        "Please provide a meaningful and concise topic description for this cluster.\"\n",
        "\n",
        "Refined Topic Representations:\n",
        "\n",
        "* Cluster 0: Deuteron-Induced Nuclear Reactions in Metal Deuterides\n",
        "* Cluster 1: Cold Fusion/Low Energy Nuclear Reactions (LENR) in Palladium\n",
        "* Cluster 2: Electrolytic Cell Reactions and Excess Heat:  Studies on electrolysis with various electrolytes (e.g., LiOD, LiOH, NaF, K2CO3) and electrodes (e.g., Pt, Pd, Ni) focusing on excess heat production and potential fusion reactions.\n",
        "* Cluster 3: Cold Fusion in Metal Deuterides: Theoretical and Computational Studies\n",
        "* Cluster 4: Cold Fusion Controversy and Research"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oz28BxEiqAtP"
      },
      "source": [
        "# **ASSIGNING NEW DOCUMENT TO AN EXISTING CLUSTER**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5Y5M_A7qJ5R"
      },
      "source": [
        "# **1. Load embedding vectorizer, cluster names, and K-Means model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "IYo-uvvQqINd"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "VECTORIZER_PATH = os.path.join(OUTPUT_DIR, \"tfidf_vectorizer.pkl\")\n",
        "CLUSTER_SUMMARY_PATH = os.path.join(OUTPUT_DIR, \"refined_cluster_summary.csv\")\n",
        "\n",
        "# Load vectorizer\n",
        "with open(VECTORIZER_PATH, \"rb\") as f:\n",
        "    vectorizer = pickle.load(f)\n",
        "\n",
        "# Load cluster summaries with names\n",
        "df_cluster_summary = pd.read_csv(CLUSTER_SUMMARY_PATH)\n",
        "\n",
        "# Load K-Means model\n",
        "with open(os.path.join(OUTPUT_DIR, \"kmeans_model.pkl\"), \"rb\") as f:\n",
        "    kmeans = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5rAWbEvrYFN"
      },
      "source": [
        "# **2. Load and preprocess new document (JSON)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ZIAlUY2kreEI"
      },
      "outputs": [],
      "source": [
        "def load_and_preprocess_new_json(json_path):\n",
        "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    title = data[\"metadata\"].get(\"title\", \"\") or \"\"\n",
        "    abstract = \" \".join([para.get(\"text\", \"\") for para in data[\"metadata\"].get(\"abstract\", []) if para.get(\"text\")])\n",
        "    body_text = \" \".join([para.get(\"text\", \"\") for para in data[\"metadata\"].get(\"body_text\", []) if para.get(\"text\")])\n",
        "    full_text = f\"{title} {abstract} {body_text}\".strip()\n",
        "\n",
        "    if not full_text:\n",
        "        raise ValueError(\"Document is empty.\")\n",
        "\n",
        "    return preprocess_text(full_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvWJUuJRswoF"
      },
      "source": [
        "# **3. Vectorize new doc using TF-IDF**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9xOIX5wNs2KA"
      },
      "outputs": [],
      "source": [
        "def vectorize_text(text, vectorizer):\n",
        "    return vectorizer.transform([text]).toarray()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-YAzGLXs4wJ"
      },
      "source": [
        "# **4. Assign to cluster and report distances**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "N2ONLirZs9s3"
      },
      "outputs": [],
      "source": [
        "def assign_to_cluster(tfidf_vec, kmeans_model):\n",
        "    from sklearn.metrics.pairwise import euclidean_distances\n",
        "    distances = euclidean_distances(tfidf_vec, kmeans_model.cluster_centers_).flatten()\n",
        "    closest_cluster = distances.argmin()\n",
        "    return closest_cluster, distances"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cao_ZjSltJ0v"
      },
      "source": [
        "# **5. Output assigned cluster name and all distances**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "hxU97K3FtJl7"
      },
      "outputs": [],
      "source": [
        "def report_assignment(cluster_id, distances, df_cluster_summary):\n",
        "    print(\"\\n Distance to each cluster:\")\n",
        "    for i, dist in enumerate(distances):\n",
        "        print(f\"  Cluster {i}: {dist:.4f}\")\n",
        "\n",
        "    cluster_name = df_cluster_summary.loc[df_cluster_summary[\"Cluster\"] == cluster_id, \"Refined Topic\"].values[0]\n",
        "    print(f\"\\n Assigned to Cluster {cluster_id}: \\\"{cluster_name}\\\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAAMjrqftlNC"
      },
      "source": [
        "# **6. Running the pipeline**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJd_fPU0tpDt",
        "outputId": "14f44401-f612-4447-80b8-d3420503df49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Assigning document: /content/drive/My Drive/BDS_Topic_Clustering/new_pdfs_mds/4788/4788_combined.json\n",
            "\n",
            " Distance to each cluster:\n",
            "  Cluster 0: 1.2339\n",
            "  Cluster 1: 0.9874\n",
            "  Cluster 2: 1.0450\n",
            "  Cluster 3: 1.0202\n",
            "  Cluster 4: 1.0043\n",
            "\n",
            " Assigned to Cluster 1: \"Cold Fusion/Low Energy Nuclear Reactions (LENR) in Palladium\"\n",
            " Embedding saved to /content/drive/My Drive/BDS_Topic_Clustering/fresh_output_embeddings/new_doc_embedding/4788_combined_embedding.npy\n"
          ]
        }
      ],
      "source": [
        "def assign_document_to_cluster(json_path, vectorizer, kmeans_model, df_cluster_summary, save_dir=None):\n",
        "    print(f\"\\n Assigning document: {json_path}\")\n",
        "    text = load_and_preprocess_new_json(json_path)\n",
        "    tfidf_vec = vectorize_text(text, vectorizer)\n",
        "    cluster_id, distances = assign_to_cluster(tfidf_vec, kmeans_model)\n",
        "    report_assignment(cluster_id, distances, df_cluster_summary)\n",
        "    if save_dir:\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        file_name = os.path.basename(json_path).replace(\".json\", \"_embedding.npy\")\n",
        "        np.save(os.path.join(save_dir, file_name), tfidf_vec)\n",
        "        print(f\" Embedding saved to {os.path.join(save_dir, file_name)}\")\n",
        "\n",
        "assign_document_to_cluster(\n",
        "    json_path=\"/content/drive/My Drive/BDS_Topic_Clustering/new_pdfs_mds/4788/4788_combined.json\",\n",
        "    vectorizer=vectorizer,\n",
        "    kmeans_model=kmeans,\n",
        "    df_cluster_summary=df_cluster_summary,\n",
        "    save_dir=os.path.join(OUTPUT_DIR, \"new_doc_embedding\")\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1934c82949ad45249c1467ef04f494cd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27a1d8449c0a4e22bb4b86bb0eefed1f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3003a5cf84f048ce97985e2cdb6c73b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "49a841099edc437f97a645854cb066a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7674bfca28244dbba8d086b8f060ee3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5d202508e9041b8a6b018e64aaef8f1",
            "max": 11573070,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3003a5cf84f048ce97985e2cdb6c73b2",
            "value": 11573070
          }
        },
        "9c016d00468944a78fc29406d96d080f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1934c82949ad45249c1467ef04f494cd",
            "placeholder": "​",
            "style": "IPY_MODEL_49a841099edc437f97a645854cb066a0",
            "value": " 11.6M/11.6M [00:00&lt;00:00, 104MB/s]"
          }
        },
        "9ea661c9e74d4cd1b9c6db2b7af8a3ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_acac42bf9a58430f9ded1727c3fe3692",
              "IPY_MODEL_7674bfca28244dbba8d086b8f060ee3a",
              "IPY_MODEL_9c016d00468944a78fc29406d96d080f"
            ],
            "layout": "IPY_MODEL_c238b39654004f03b33d9d2e1e2e3132"
          }
        },
        "a5d202508e9041b8a6b018e64aaef8f1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "acac42bf9a58430f9ded1727c3fe3692": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27a1d8449c0a4e22bb4b86bb0eefed1f",
            "placeholder": "​",
            "style": "IPY_MODEL_c64ec0f9ebe746b3bc08c24b49f759fc",
            "value": "densenet_lite_136-gru-onnx.zip: 100%"
          }
        },
        "c238b39654004f03b33d9d2e1e2e3132": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c64ec0f9ebe746b3bc08c24b49f759fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
